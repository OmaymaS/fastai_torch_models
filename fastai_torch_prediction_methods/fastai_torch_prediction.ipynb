{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c51afb-f3e0-48c2-ab6b-74023b26e1b4",
   "metadata": {},
   "source": [
    "# fastai vs. native torch predictions\n",
    "\n",
    "fastai is a high level API built on top of torch providing features to simplify training models. Everything written in fastai can be converted to native torch. This could be useful if a decision is made to reducde dependencies in production *(e.g. prediction part)* .\n",
    "\n",
    "The following sections explain the parallels between fastai and torch, focusing on the **prediction part**. The versions of the fastai/torch are and included in `requirements_fastai.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3db3d9f6-fef9-4ab1-af0e-2d2d590d6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset_custom import ImageDataset ## custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25367ef2-0ed4-46dd-be18-7a9d0d029e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using num_workers=0 for local testing (but can be changed to more if desired)\n",
    "PRED_NUM_WORKERS = 0\n",
    "\n",
    "## specify model and test data\n",
    "model_path = 'models'\n",
    "model_name = 'model_modified.pkl'\n",
    "test_data_path = 'images/imgs_zz/' ## small sample "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e44d63-37a1-4119-9117-38c3d317ffe8",
   "metadata": {},
   "source": [
    "## fastai (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98058ddf-f836-4c8f-a76d-13e6e9f2c713",
   "metadata": {},
   "source": [
    "We'll start by using a model that has already been trained and exported as `.pkl` using fastai (2.5.5).\n",
    "\n",
    "### Loading models in fastai\n",
    "\n",
    "To load models in fastai, `load_learner()` is used. The loaded model includes more details than models exprorted in native torch. For instance, the dataloader object includes the transformations specified at the time of model training and other details such as the list of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8615066-6233-44b1-a92b-4331a5747798",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model \n",
    "learn_multi = load_learner(f'{model_path}/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d29e5e-3ee9-47a6-81dc-65784f84e026",
   "metadata": {},
   "source": [
    "### Batch prediction in fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a1342-8576-46ef-a932-5d0d13a923ac",
   "metadata": {},
   "source": [
    "To predict on a batch, the following steps should be followed:\n",
    "\n",
    "- get paths of images.\n",
    "- load images in batches and the corresponding transformation using `test_dl`.\n",
    "- predict on batches and return all predictions.\n",
    "\n",
    "\n",
    "Note that the default batch size is 64, but we pass another value to `bs`. his value can be retrieved using `tst_dl.bs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d36a41fa-89ca-4ecc-99b6-e0521f9a398a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## predict on batch\n",
    "tst_files = get_image_files(test_data_path)\n",
    "tst_dl = learn_multi.dls.test_dl(tst_files, bs=4, num_workers=PRED_NUM_WORKERS)\n",
    "preds_fastai, _ = learn_multi.get_preds(dl=tst_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf5973-0764-40c7-a1c2-56cb1a0e596d",
   "metadata": {},
   "source": [
    "### Data in loaded model and dataloader (details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7029f-b94d-459d-af57-ba54ebd85586",
   "metadata": {},
   "source": [
    "#### Tags names\n",
    "\n",
    "To retrieve tag names with the order returned by the predictions functions, use `learn_multi.dls.vocab` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68f0fdf8-5d69-41da-b745-0a79bd6c2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "learn_multi.dls.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81cede-d9be-4988-b8e1-6607134b16e7",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "\n",
    "To retrieve the image transformations applied on the images, check:\n",
    "- `tst_dl.after_item` (applied on each item)\n",
    "- `tst_dl.after_batch` (applied on each batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f2f78-987a-4fbb-8351-2142db1903f7",
   "metadata": {},
   "source": [
    "**\\*.after_item()**\n",
    "\n",
    "Looking at the item transforms, we can see that the image gets resized to 460x460, with method squish for train data. However, aaccording to the [docs](https://docs.fast.ai/vision.augment.html#Resize), valid or test data always get center cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ff94c33-5e9f-4d80-850e-85f4893173f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: Resize -- {'size': (460, 460), 'method': 'squish', 'pad_mode': 'reflection', 'resamples': (2, 0), 'p': 1.0} -> ToTensor"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dl.after_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ac48c-c2e9-49c4-8a5c-f0c318aedb27",
   "metadata": {},
   "source": [
    "**\\*.after_batch()**\n",
    "\n",
    "The batch transformtions here are mainly `aug_transforms()`, which is applied on training data and not relevant to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544d8a43-861d-48a8-b597-7de4919dda3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Warp -- {'magnitude': 0.2, 'p': 1.0, 'draw_x': None, 'draw_y': None, 'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'batch': False, 'align_corners': True, 'mode_mask': 'nearest'} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dl.after_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ce936-cc1f-4182-bcc7-7e312e4123fb",
   "metadata": {},
   "source": [
    "So what we care about here is the resize value which gets applied to train/valid/test data --> `tst_dl.after_item.size` or `learn_multi.dls.after_item.size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b170ecd-80e8-4620-a8e4-d9131eec72a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 460)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dl.after_item.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a08a22-977d-431f-980e-b44b6dfce994",
   "metadata": {},
   "source": [
    "\n",
    "According to these transformations, what we expect from the dataloader includes the following steps:\n",
    "\n",
    "- resizing image *(to `learn_multi.dls.after_item.size`)*.\n",
    "- transforming to tensor *(expected to see normalized values in the range [0,1])*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea2566-5ef1-43f4-b57a-008f432d1210",
   "metadata": {},
   "source": [
    "### Zoom in on one example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721b898-cb8a-45e7-950b-b07d1c5eaea4",
   "metadata": {},
   "source": [
    "Looking closer at one loaded test image `tst_dl.items[3]`, we can see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a727a04-b058-40e4-b921-8f07ffa3e4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('images/imgs_zz/344922.jpeg')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## image path \n",
    "tst_dl.items[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2b8d578-0191-450f-b178-1d830c78d555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loaded image type\n",
    "tst_dl.one_batch()[0][3].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bab122f-13a7-4e86-9156-0128df273a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PILImage mode=RGB size=800x600,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## image size before resizing\n",
    "tst_dl.dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b37b6-6beb-406a-92a7-3e685c787dd4",
   "metadata": {},
   "source": [
    "The size is similar to what's expected `learn_multi.dls.after_item.size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3712604-1fc3-41d2-87a9-dca66cb1d88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 460, 460)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## image size after resizing \n",
    "test_image_sample_fastai = np.array(tst_dl.one_batch()[0][3])\n",
    "test_image_sample_fastai.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e8023-785c-48d4-a87d-6707c12d535f",
   "metadata": {},
   "source": [
    "The range of values is between [0,1] *(checking one channel)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b099564-b818-4519-96fa-1613370d1bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loaded image --> tensor values range\n",
    "test_image_sample_fastai.min(), test_image_sample_fastai.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea652120-aafe-42fb-964c-9e152143bd2c",
   "metadata": {},
   "source": [
    "The predictions values combined with tag names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "289d24c7-f1ba-4842-96bf-3a7f81c1f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.2788e-04, 1.8691e-03, 2.6241e-04, 1.8735e-03, 7.3655e-01, 1.1181e-03,\n",
       "        1.5422e-02, 9.6627e-05, 9.1196e-03, 6.3069e-05, 2.5095e-03, 9.8131e-05])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predictions\n",
    "preds_fastai[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111c81d-1d26-486b-b171-0c0ec0f70d90",
   "metadata": {},
   "source": [
    "## torch (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304a5d3-3630-4213-aeb8-0555693a6649",
   "metadata": {},
   "source": [
    "Now let's see the equivalent in native torch! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3a6e6-72b4-4f02-8143-659073351c13",
   "metadata": {},
   "source": [
    "### Saving fastai models for torch (jit)\n",
    "First we'll save the model in a format that can be read by native torch with no dependancy on fastai as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d34e522-e56b-4372-b89d-4ec8385b4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## image size \n",
    "fastai_model_img_size = tst_dl.after_item.size[0]\n",
    "\n",
    "## save for native torch import later\n",
    "dummy_inp = torch.randn([1,3,fastai_model_img_size,fastai_model_img_size]) ## dummy \n",
    "torch.jit.save(torch.jit.trace(learn_multi.model, dummy_inp), 'models/model_modified_jit.pt')\n",
    "\n",
    "## save vocab with model \n",
    "vocab = np.save('models/vocab.npy', learn_multi.dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1b288-b8e9-4a0e-8d84-0bc66d518058",
   "metadata": {},
   "source": [
    "### Loading models in torch\n",
    "In practice, if we are just writing the prediction part, we'll start from this point where a model already exists \n",
    "Here we'll try to write the equivalent prediction code without using any functions from fastai. Since the fastai model was exported using `jit`, we'll use `torch.jit.load()`. Then we'll use `eval()` which is an important step before prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "049605c8-7f51-4aaf-b3cf-5f369e66695a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## load model\n",
    "model_loaded = torch.jit.load('models/model_modified_jit.pt')\n",
    "\n",
    "## eval() is required after load() during inference/prediction\n",
    "_ = model_loaded.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20424b88-fa6d-440f-879b-ee28fab3e2b4",
   "metadata": {},
   "source": [
    "Since the model is not a fastai model anymore, we won't have access to the resizing value, transformations or vocab. We should have these values saved when the model gets exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32329cfc-e950-44a4-953e-81faf355b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify resize value (or retrieve from a file if saved)\n",
    "IMG_RESIZE_VALUE = 460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a2386b6c-e2ca-4a56-b4ee-ee3d0d6ad363",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specifiy test images\n",
    "test_files = glob.glob(f'{test_data_path}*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0202b3-fbba-4dc1-adaf-444bd9c34280",
   "metadata": {},
   "source": [
    "### Transformation and prediction in torch (one example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a408dcb-4406-432a-ad07-786d4aa453e2",
   "metadata": {},
   "source": [
    "Here we'll take the same sample image `test_files[3]` which corresponds to `tst_dl.items[3]`. We'll apply the transformations and predict using torch as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "667ce5a9-8fef-47d1-8408-121ee4f21154",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): ## important to use during prediction\n",
    "    pil_image = Image.open(test_files[3]) # open image as PIL\n",
    "    resize = T.Resize([IMG_RESIZE_VALUE, IMG_RESIZE_VALUE]) # specify resize value\n",
    "    res_pil_image = resize(pil_image) # use torch native resize \n",
    "    timg = T.ToTensor()(res_pil_image) # convert to tesnor\n",
    "    pred_sample_torch = model_loaded(timg.unsqueeze(0)).sigmoid() # use model to get predictions (sigmoid for multi-label multi-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485d7b3-eed0-4dd9-b51c-f47fd100bda5",
   "metadata": {},
   "source": [
    "Looking at the results, we can see the same predictions as fastai predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "804266cd-6670-4024-a78a-2b33da9958ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.2788e-04, 1.8691e-03, 2.6241e-04, 1.8735e-03, 7.3655e-01, 1.1181e-03,\n",
       "        1.5422e-02, 9.6627e-05, 9.1196e-03, 6.3069e-05, 2.5095e-03, 9.8132e-05])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sample_torch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a6d19-fd27-47ac-9d44-8abcf76746dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batch prediction in torch \n",
    "\n",
    "To predict on a batch, we can use a [custom dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) to include all transformations as shown below. If we inspect the`ImageDataset` class under `dataset_custom.py`, we can see the details. The main point is adding the transformations to `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da779649-d323-4c71-8726-82d976ea341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "dataset = ImageDataset(test_files, img_size=IMG_RESIZE_VALUE)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=PRED_NUM_WORKERS) #iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f0f94-0076-4d13-b244-fa8d52ff284c",
   "metadata": {},
   "source": [
    "In case of datasets including images > `batch_size`, we need to loop over the batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dfa5d7d4-99e6-42e6-ab12-7f153dda5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        preds = model_loaded(batch)\n",
    "        preds_batch = preds.sigmoid()  \n",
    "        final_output.append(preds_batch)\n",
    "preds_torch = torch.cat(final_output, dim=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dded924-f616-4148-bb53-31677acae7ea",
   "metadata": {},
   "source": [
    "## fastai versus torch predictions\n",
    "\n",
    "Comparing the probablities from fastai versus torch, we can see that we got the same values.\n",
    "*Note that the values should be the same, but might have slight differences after a certain N decimal points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faeae5cc-8fd7-46bd-976f-e5eb7ab795cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.around(preds_torch.numpy(), 3), np.around(preds_fastai.numpy(), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_fastai_real",
   "language": "python",
   "name": ".venv_fastai_real"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
